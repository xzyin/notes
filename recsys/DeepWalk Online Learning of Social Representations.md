# Abstract
&emsp;&emsp;我们提出了一种新的学习隐向量表示的方法——Deep walk，用来学习网络中节点的隐含表示。这些隐含表示在一个连续的向量空间中编码了社会化关系。Deep-Walk采用了最近在语言模型和无监督特征学习上的一些优点，将隐向量的表示从词汇序列应用到图上面。

&emsp;&emsp;Deep Walk使用随机游走截断得到的局部信息来学习向量的隐含表示,并且将随机游走截断得到的路径看成是语言模型中的句子进行训练从而得到向量表示。我们展示了Deep walk在Blog-Catalog, Flickr以及YouTube等社交网络分类任务中的表现。我们的实验结果显示Deep walk表现优于具有挑战性的一些其他Baseline,Deep Walk允许对网络有一个全局的视野，尤其是在缺失数据的情况下。Deep Walk的向量表示能够在标签稀疏的情况下使得我们得到的 $F_1$ 的值比原来的结果要高 $10\%$  在一些实验中Deep walk的表示能够只使用少于60%的数据但是比所有的Baseline模型的表现都要好。

&emsp;&emsp;Deep walk同样是可扩展的，是支持建立有效的增量更新结果的在线学习方法并且非常容易支持并行化。这种特性使得Deep walk能够更好地应用于现实世界,例如网络分类和异常检测。

#1 Introduction
&emsp;&emsp;网络的稀疏性表示竟是一个优点也是一个缺点。网络的稀疏性使得我们能够高效地设计一些离散的算法，但是同样的特性使得网络结构很难引入到统计模型的学习。机器学习模型为了能够应用到网络结构中(例如：内容推荐,异常检测,缺失值预测等)必须解决网络结构的稀疏性问题。

&emsp;&emsp;在这篇文章中,我们引入了在自然语言处理领域已经证明有效的深度学习(非监督特征学习)技术来进行网络结构分析。我们提出了一个Random Walk算法通过建模一个用来学习图节点的社会化表示。社会化表示是能够捕捉节点相似度和社区关系的隐含特征。这些隐向量通过一个相对较小的维度在连续空间中编码了社会化关系。Deep Walk将神经网络模型一般话,以通过一组随机生成的行走组成的特殊语言。这些语言模型通常被用于捕获人类语言中的句法和结构甚至是一些逻辑句段。

&emsp;&emsp;Deep Walk以一个图作为输入并且生成一个隐含向量的表示。在图1a中展示了我们的模型应用在空手道数据集上的结果。在图中使用force-directed布局展示了整个图。在图1b中我们使用一个二维的隐含向量我们方法的输出。除了节点之间惊人的相似性之外，对于输入图1a我们可以找到一个线性分离器能够分离对应的簇从而找到最大化的模块。

&emsp;&emsp;为了展示Deep Walk在现实世界中的潜力，我们评估了Deep Walk在大型异构网络中挑战多标签网络分类问题的性能。在相关的分类问题中，特征之间的关系违反了独立同分布假设。解决这个问题通常采用近似推理的方式来最大化的使用依赖的信息从而最大化的提高分类的结果。我们通过独立于标签的图形表示从而避免这些方法的使用。因此我们的表示质量不受我们选择的标签节点的影响，因此这些标签的表示可以在任何任务之前进行共享。

&emsp;&emsp;Deep Walk在创建社交属性维度上，Deep Walk相对于其他向量表示的方法具有更好的性能，尤其在标签节点稀疏的情况下。采用我们的线性表示使得在简单的线性分类器上也能够有很好的效果。我们的向量表示具有更强的泛化能力并且可以和其他任何分类任务进行关联。Deep Walk不仅实现了并且是一个实时的算法能够支持较高的并行度:

&emsp;&emsp;我们的贡献如下:

* 我们介绍了一种深度学习方法作为分析图的工具，以构建适用于统计模型的健壮的向量表示。Deep Walk在短期的随机游走中学习展现出来的结构化规律。
* 我们几个社会化网络的多标签任务中大量评估了我们的标签向量的表示。在标签稀疏性的情况下，我们的结果在分类任务上有明显的性能提升，在我们能够考虑的最稀疏的情况下 $F1$ 的得分上大概有5%~10%的提升。在某些情况下，即使训练数据减少60% Deep walk的训练效果也能战胜对手。
* 我们通过并行的实现在web规模的网络数据上证明了算法的可扩展性，并且我们在此基础之上描述了构架流式版本所需要的最小改动。

&emsp;&emsp;在剩余的部分我们对论文的组织结构如下。在Section 2和Section 3中,我们讨论了网络中分类问题的形式化描述，以及这些工作跟我们的工作的关系。在Section 4中提出了我们针对于社会化表示学习的Deep Walk方法。在Section 5中对我们的相关实验做了一个概述，在Section 6中我们展示了我们的实验结果。最后在Section 7中我们提出了相关工作的讨论以及我们的结论。

# 2 PROBLEM DEFINITION
&emsp;&emsp;在这篇文章中，我们考虑将社交网络成员的分类成为一个或者多个类的问题。更形式化的描述为，对于图 $G = (V,E)$ 其中 $V$是网络节点中的成员，并且 $E$ 是网络中的边, 并且有 $E \subseteq (V \times V)$ 给出部分标记的社会化网络 $G_L=(V,E,X,Y)$。其中 $X \in \mathbb{R}^{|V| \times S}$ 对于每个特征向量来说 $S$ 是特征空间的大小，并且 $Y \in \mathbb{R}^{|V| \times |U|}$ 其中 $U$ 是标签的集合。

&emsp;&emsp;在一个传统的机器学习分类场景中，我们的目的主要是为了学习一个假设H能够将元素X映射到标签集合Y。在我们的case中，我们可以通过使用给出例子的一些重要信息来实现对图G的高效嵌入。

&emsp;&emsp;我们提出了一个不同的方法来捕获网络的拓扑信息，我们提出了一种无监督的方法来学习独立于标签分布的图结构特征，而不是将标签空间作为特征空间的一部分，结构表示和标记之间的分离避免了级联错误，而级联错误通常可能在迭代方法中发生。而且同样的向量表示能够用在网络相关的不同任务中。

&emsp;&emsp;我们的目的在于学习每个成员的Embedding向量 $X_E \in \mathbb{R}^{|V| \times \mathcal{Y}}$, 其中 $\mathcal{Y}$ 是较小的数用来表示隐式空间的维度。这种低维的表示是分布式，meaning  each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space.

&emsp;&emsp;使用这些结构化的特征我们可以扩大属性空间以帮助分类任务做决策。这些特征是可以普遍用于任何分类算法，但是我们认为这些特征最好的使用方式是能够和一些简易的机器学习方法做一些简单集成。我们将在Section 6中介绍这些方法在现实世界中的大规模使用。

# 3 LEARNING SOCIAL REPESENTATIONS
&emsp;&emsp;我们发现社会化学习的表示具有以下几个特性:

* <Strong>Adaptability</Strong>:适应性,真实的社交网络是在不断发展的，新的社交关系没有必要被重复的计算并且多次处理。
* <Strong>Community aware</Strong>:社区意识,隐含维度之间的距离度量能够评估社交网络中对应成员之间的关系。这允许同质性的网络能够进行一般化地概括。
* <Strong>Low dimensional</Strong>:低纬度,当被标记的数据稀缺时,低维模型能够能够更好的泛化加速收敛和推断。
* <Strong>Continuous</Strong>:连续性,我们在连续的隐含空间中建模部分网络的成员的关系，除了能够表示成员之间细微的差别之外,在社区之间具有平滑的决策边界允许能够得到更加健壮的分类。

&emsp;&emsp;我们使用语言模型的中相关的优化技术通过从一个短的随机游走从stream中学习图节点的表示从而满足上述的特性，接下来我们回顾一下随机游走和语言模型并且介绍如何连接这两种方法。
## 3.1 Random Walk
&emsp;&emsp;我们将一个根节点为 $v_i$ 的随机游走表示为 $\mathcal{W}_{v_i}$,这是一个带有随机变量 $\mathcal{W}_{v_i}^{1},\mathcal{W}_{v_i}^{2},\mathcal{W}_{v_i}^{k}$ 的随机过程，在这个过程中 $\mathcal{W_{v_i}^{k+1}}$ 表示从节点 $v_k$ 的邻居节点中随机选取的一个变量。在大量的内容推荐问题和社区发现问题中随机游走序列被作为常用的内容相似度量方法。这些随机游走序列是输出敏感性算法的基础，通过使用这些序列在图大小的次线性复杂度下来计算局部社区的结构信息。
&emsp;&emsp;正是随机游走序列局部结构联系促使我们使用短的随机游走序列作为基础方法来处理网络中已经存在的结构信息。此外为了捕获社区信息，使用随机游走作为我们算法的基础给了我们两个其他的理想的特性。首先，局部的探索很容易地能够进行并行化。几个随机游走的walker(在不同线程,进程或者是机器)能够同时探索同一个图的不同部分。其次，Deep Walk算法依赖短的随机游走序列能够获取信息，从而不需要全局计算已更新信息。对于整个图来说，我们可以通过从改变的局部区域得到新的随机游走序列迭代地更新模型。

## 3.2 Connection：Power laws
&emsp;&emsp; 竟然选择了随机游走序列用来捕获图形的原始结构,我们现在需要一个合适的方法来捕获这些信息。如果一个连接的图的度分布满足幂律分布(无标度图)，我们能够观察到的是节点出现在短的随机游走序列中的时候也遵循幂律分布。

&emsp;&emsp;在自然语言处理中词频分布同样复合幂律分布，并且语言模型解释了这种分布行为。为了强调这种分布上的相似性，我们在两张图中展示了两个幂律分布，如图2所示，第一个图表示的是在无标度图中的随机游走序列，在第二个图中表示的是10万个英文wiki文章。

&emsp;&emsp;我们主要的核心贡献就是将在自然语言建模中使用的技术迁移到了网络的社区结构(在两种模型中，数据同样的具有幂律分布的特性)。我们在接下来的工作中review了语言模型中的相关建模的方法，并且将这些方法转换到我们相关场景下的节点向量表示。

## 3.3 Language Modeling
&emsp;&emsp;语言模型的目的建立在语料库中出现的词汇的相似性。形式化的描述如下：
$$W_1^n = (\omega_o, \omega_1,..., \omega_n)$$
其中 $\omega_i \in \mathcal{V}$( $\mathcal{V}$ 是语料库的词汇表), 我们在整个训练集上训练的目的就是为了优化 $Pr( \omega_n | \omega_0, \omega_1,...\omega_{n-1})$
